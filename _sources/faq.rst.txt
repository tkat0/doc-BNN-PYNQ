FAQ
====

BNN-PYNQのソースコードは5/2時点で最新のものを使いました。

https://github.com/Xilinx/BNN-PYNQ/tree/a86e0863418ce4161ed61b69ba89ec1481014362


BNN-PYNQってなに？
-------------------

ホビーFPGAボード"PYNQ(ピンク)"で動作するNeuralNetworkのシステム。
行列演算などをハードウェア化することで、DeepLearningアルゴリズムを、CPUより高速化（アクセラレーション）する試みが、至る所で行われています。これもその一種。


アプリケーションは、例えば以下。

https://github.com/Xilinx/BNN-PYNQ/blob/master/notebooks/Cifar10.ipynb

BNN-PYNQは同じNeuralNetworkをCPUとFPGAそれぞれで動かせるようになっていて、「CPUよりFPGA化したほうがこんなに速いよ」という内容が上記でも確認できます。

BNN-PYNQは、以下のような工夫をして高速化しています。
詳細は `論文 <https://arxiv.org/abs/1612.07119>`_ に記載があります。

* 演算精度のバイナリ化（0,1）
    * メモリの使用量、転送量を削減
    * ビット演算器を活用した高速化
* 演算の並列化

学習も推論もFPGAでやるの？
-----------------------------

No。学習はPCで行い推論をFPGAで実行する仕組み。

PCのDeepLeraningフレームワークで学習したモデルを、BNN-PYNQ用のデータフォーマットに変換するスクリプトは付属しています。
現時点でTheanoで学習したパラメータの変換に対応しているようです。
サンプルは、Lasagne（Theanoをバックエンドにできる）で記述されています。

データフォーマットの変換だけなので、ちょいとスクリプトを書けば、TensorFlowやChainerなどとも連携できるはずです。(今度のイベント向けにやってみようかな)

認識精度は？
--------------

付属の学習済みモデルに関する認識率は、ここに記載があります。

https://github.com/Xilinx/BNN-PYNQ/tree/master/bnn/src/training#training-networks

バイナリ化しない場合に比べて、GPUでの学習時間がすこし長い印象がありますが、どうなんでしょうか？


PCで動く任意のモデルをFPGA化できるの？
----------------------------------------

No。現状はLFCとCNVと呼ばれる2種類のモデルに対応してます。
例えばLSTMなど含むモデルは動かせません。

ただし、BNN-PYNQが対応しているレイヤーで構成されるモデルであれば、FPGAのリソースに乗る範囲でレイヤーを組み替えて任意のモデルは作れると思います。

BNN-PYNQでは、バイナリ化したConvolutionやFullyConnectedレイヤーのハードウェアを使います。
そのため、PC側もそれに合わせてバイナリ化したレイヤーを定義して学習しないと、学習した重みをFPGAに乗せられません。
通常のPC側のフレームワークだとfloat(32bit)で計算する部品がほとんどです。
そのため、BNN-PYNQでは、PCのフレームワーク用にもバイナリ化したレイヤーを定義して、それを用いてモデルを作り、学習しています((元祖 Binarized Neural Networksの論文の実装をベースにしてるぽいですね))。

もし、ConvolutionやFullyConnected以外を使いたい場合は、ハード側にそのレイヤーを実装するか、そのレイヤーをソフト処理するかだと思います。
しかし、ソフト処理は現実的ではないと思います。BNN-PYNQは推論開始後、最期のレイヤが計算完了するまで制御がCPUに戻らない仕組みです。そのため、途中の一部のレイヤだけソフト処理にするのはBNN-PYNQのコアの修正が必要で、できたとしても速度が遅くなると思います。
そのため、追加したいレイヤーのCPUよりも高効率なハードウェアアルゴリズムを考えて、それを高位合成なりでハード化するのが面白いんじゃないでしょうか??


任意のデータで既存のモデル（LFCとCNV）を再学習できるの？
----------------------------------------------------------

Yes

CNV, LFCは それぞれ学習データのフォーマットが決まっているので、それに従い作成したデータなら学習可能です。

任意のタスクにBNN-PYNQを応用する場合は、まずは既存のネットワーク（LFC,CNV）を使うところから考えるのが良さそうですね。


CPUとアクセラレータ間のデータ転送のタイミングは？
-------------------------------------------------

データのコピーが性能上のボトルネックになるので、どのタイミングでコピーしているかは重要です。
ソースを見ると、以下のタイミングでコピーが発生しているようです。

1. モデル初期化時に、全レイヤーのパラメータをCPUからアクセラレータへコピー
2. 推論開始時に、入力画像データを、CPUからアクセラレータへコピー
3. 推論終了時に、推論結果をアクセラレータからCPUへコピー

推論中は、アクセラレータとメモリ内で閉じて演算しているようです。

